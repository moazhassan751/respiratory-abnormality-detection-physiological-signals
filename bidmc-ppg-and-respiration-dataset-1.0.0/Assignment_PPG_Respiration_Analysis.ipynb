{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da8611e",
   "metadata": {},
   "source": [
    "# Assignment: Statistical Analysis of PPG and Respiration Dataset\n",
    "\n",
    "---\n",
    "\n",
    "**Student Name:** [Your Name Here]  \n",
    "**Student ID:** [Your ID Here]  \n",
    "**Date:** November 17, 2025  \n",
    "**Course:** Biomedical Signal Processing  \n",
    "**Dataset:** BIDMC PPG and Respiration Dataset v1.0.0\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Objectives:\n",
    "\n",
    "1. ‚úÖ Load and preprocess physiological signal data (PPG/Respiration)\n",
    "2. ‚úÖ Perform exploratory data analysis with visualization\n",
    "3. ‚úÖ Check Gaussian distribution using histogram analysis\n",
    "4. ‚úÖ Conduct Shapiro-Wilk Normality Testing\n",
    "5. ‚úÖ Extract Time-Domain Features (9 features per signal)\n",
    "6. ‚úÖ Extract Frequency-Domain Features (6 features per signal)\n",
    "7. ‚úÖ Provide comprehensive feature selection hypothesis\n",
    "8. ‚úÖ Generate analysis report with visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Information:\n",
    "\n",
    "- **Total Subjects:** 53 (bidmc01 to bidmc53)\n",
    "- **Recording Duration:** ~8 minutes per subject\n",
    "- **Sampling Rates:** 125 Hz (signals), 1 Hz (numerics)\n",
    "- **Signals:** RESP, PLETH (PPG), ECG (V, AVR, II)\n",
    "- **File Types:** .dat, .hea, .breath, CSV files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183effd",
   "metadata": {},
   "source": [
    "## üìã SUBMISSION INSTRUCTIONS\n",
    "\n",
    "### For Google Colab Submission:\n",
    "\n",
    "1. **Upload Dataset to Google Drive:**\n",
    "   - Upload the `bidmc-ppg-and-respiration-dataset-1.0.0` folder to your Google Drive\n",
    "   - Ensure it contains the `bidmc_csv` subfolder\n",
    "\n",
    "2. **Update Dataset Path:**\n",
    "   - In the setup cell below, change `DATASET_BASE_PATH` to your Google Drive path\n",
    "   - Example: `/content/drive/MyDrive/YourFolderName/bidmc-ppg-and-respiration-dataset-1.0.0`\n",
    "\n",
    "3. **Run All Cells:**\n",
    "   - Click \"Runtime\" ‚Üí \"Run all\" in Google Colab menu\n",
    "   - Verify all outputs are generated\n",
    "\n",
    "4. **Share Notebook:**\n",
    "   - Click \"Share\" button in top-right\n",
    "   - Set to \"Anyone with the link can view\"\n",
    "   - Copy the shareable link\n",
    "\n",
    "5. **Submit:**\n",
    "   - Submit the Google Colab link on Google Classroom\n",
    "   - Ensure the notebook shows all executed cells with outputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d0e7e",
   "metadata": {},
   "source": [
    "## üöÄ STEP 1: Environment Setup and Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup - Mount Drive and Configure Paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running on Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # ‚ö†Ô∏è IMPORTANT: UPDATE THIS PATH TO MATCH YOUR GOOGLE DRIVE STRUCTURE\n",
    "    DATASET_BASE_PATH = \"/content/drive/MyDrive/bidmc-ppg-and-respiration-dataset-1.0.0\"\n",
    "    CSV_PATH = f\"{DATASET_BASE_PATH}/bidmc_csv\"\n",
    "    \n",
    "    print(f\"üìÅ Dataset path: {DATASET_BASE_PATH}\")\n",
    "    print(f\"üìÅ CSV path: {CSV_PATH}\")\n",
    "    \n",
    "    # Verify dataset exists\n",
    "    if os.path.exists(DATASET_BASE_PATH):\n",
    "        print(\"‚úÖ Dataset found!\")\n",
    "        if os.path.exists(CSV_PATH):\n",
    "            print(\"‚úÖ CSV directory found!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  CSV directory not found. Please check path.\")\n",
    "    else:\n",
    "        print(\"‚ùå Dataset not found!\")\n",
    "        print(\"üìù Please upload dataset to Google Drive and update DATASET_BASE_PATH above\")\n",
    "        \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úÖ Running locally\")\n",
    "    DATASET_BASE_PATH = \".\"\n",
    "    CSV_PATH = \"bidmc_csv\"\n",
    "\n",
    "print(f\"\\nüîß Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777687e",
   "metadata": {},
   "source": [
    "## üîß STEP 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0188eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")\n",
    "print(f\"   SciPy available: Yes\")\n",
    "print(f\"   Matplotlib configured: Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480655d6",
   "metadata": {},
   "source": [
    "## üìä STEP 3: Dataset Inventory and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Dataset Inventory Check\n",
    "print(\"=\"*80)\n",
    "print(\"BIDMC DATASET INVENTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dataset_path = Path(DATASET_BASE_PATH)\n",
    "\n",
    "# Count all file types\n",
    "file_types = {\n",
    "    '.hea': {'files': [], 'description': 'Header files (metadata)'},\n",
    "    '.dat': {'files': [], 'description': 'Binary signal data'},\n",
    "    '.breath': {'files': [], 'description': 'Breath annotations'},\n",
    "    '.csv': {'files': [], 'description': 'CSV converted data'},\n",
    "}\n",
    "\n",
    "# Scan main directory\n",
    "if dataset_path.exists():\n",
    "    for file in dataset_path.glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            ext = file.suffix.lower()\n",
    "            if ext in file_types:\n",
    "                file_types[ext]['files'].append(file.name)\n",
    "\n",
    "# Scan CSV directory\n",
    "csv_path = Path(CSV_PATH)\n",
    "if csv_path.exists():\n",
    "    for file in csv_path.glob(\"*.csv\"):\n",
    "        file_types['.csv']['files'].append(f\"csv/{file.name}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìÅ FILE TYPE SUMMARY:\")\n",
    "print(\"-\" * 70)\n",
    "for ext, info in file_types.items():\n",
    "    print(f\"{ext:10s}: {len(info['files']):3d} files - {info['description']}\")\n",
    "\n",
    "total_files = sum(len(info['files']) for info in file_types.values())\n",
    "print(f\"\\nüìä TOTAL FILES: {total_files}\")\n",
    "\n",
    "# CSV breakdown\n",
    "if file_types['.csv']['files']:\n",
    "    csv_signals = [f for f in file_types['.csv']['files'] if 'Signals.csv' in f]\n",
    "    csv_numerics = [f for f in file_types['.csv']['files'] if 'Numerics.csv' in f]\n",
    "    csv_breaths = [f for f in file_types['.csv']['files'] if 'Breaths.csv' in f]\n",
    "    \n",
    "    print(f\"\\nüìÇ CSV FILES BREAKDOWN:\")\n",
    "    print(f\"   Signals:  {len(csv_signals)} files\")\n",
    "    print(f\"   Numerics: {len(csv_numerics)} files\")\n",
    "    print(f\"   Breaths:  {len(csv_breaths)} files\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset inventory complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653e9ae",
   "metadata": {},
   "source": [
    "## üì• STEP 4: Load Multi-Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866873a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from multiple subjects for robust analysis\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING MULTI-SUBJECT DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "subjects_to_analyze = ['01', '02', '03', '04', '05']  # First 5 subjects\n",
    "csv_dataset_path = Path(CSV_PATH)\n",
    "\n",
    "all_subjects_data = {}\n",
    "dataset_summary = {\n",
    "    'total_subjects': 0,\n",
    "    'total_samples': 0,\n",
    "    'signals_loaded': 0,\n",
    "}\n",
    "\n",
    "if not csv_dataset_path.exists():\n",
    "    print(f\"‚ùå CSV directory not found: {csv_dataset_path}\")\n",
    "    print(\"Please check your DATASET_BASE_PATH configuration.\")\n",
    "else:\n",
    "    for subject_id in subjects_to_analyze:\n",
    "        subject_data = {}\n",
    "        \n",
    "        # Load Signals CSV\n",
    "        signals_file = csv_dataset_path / f\"bidmc_{subject_id}_Signals.csv\"\n",
    "        if signals_file.exists():\n",
    "            subject_data['signals'] = pd.read_csv(signals_file)\n",
    "            dataset_summary['signals_loaded'] += 1\n",
    "            print(f\"‚úì Subject {subject_id}: {len(subject_data['signals']):,} samples loaded\")\n",
    "        else:\n",
    "            print(f\"‚ö† Subject {subject_id}: Signals file not found\")\n",
    "        \n",
    "        if subject_data:\n",
    "            all_subjects_data[subject_id] = subject_data\n",
    "            dataset_summary['total_subjects'] += 1\n",
    "            if 'signals' in subject_data:\n",
    "                dataset_summary['total_samples'] += len(subject_data['signals'])\n",
    "\n",
    "    print(f\"\\nüìä LOADING SUMMARY:\")\n",
    "    print(f\"   Subjects loaded: {dataset_summary['total_subjects']}\")\n",
    "    print(f\"   Total samples: {dataset_summary['total_samples']:,}\")\n",
    "    print(f\"   Signal files: {dataset_summary['signals_loaded']}\")\n",
    "\n",
    "    if all_subjects_data:\n",
    "        first_subject = list(all_subjects_data.keys())[0]\n",
    "        if 'signals' in all_subjects_data[first_subject]:\n",
    "            signal_columns = all_subjects_data[first_subject]['signals'].columns[1:]\n",
    "            print(f\"\\nüî¨ Available Signals: {', '.join(signal_columns)}\")\n",
    "        print(\"\\n‚úÖ Multi-subject data loading completed!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No data loaded. Please check file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffdd04f",
   "metadata": {},
   "source": [
    "## üîç STEP 5: Extract and Combine Signals Across Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8259709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine signals from all subjects for comprehensive analysis\n",
    "print(\"=\"*80)\n",
    "print(\"EXTRACTING AND COMBINING SIGNALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_resp_signals = []\n",
    "all_pleth_signals = []\n",
    "all_ecg_signals = []\n",
    "\n",
    "# Collect signals from all subjects\n",
    "for subject_id, data in all_subjects_data.items():\n",
    "    if 'signals' in data:\n",
    "        df = data['signals']\n",
    "        if ' RESP' in df.columns:\n",
    "            all_resp_signals.extend(df[' RESP'].values)\n",
    "        if ' PLETH' in df.columns:\n",
    "            all_pleth_signals.extend(df[' PLETH'].values)\n",
    "        if ' II' in df.columns:\n",
    "            all_ecg_signals.extend(df[' II'].values)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "resp = np.array(all_resp_signals)\n",
    "pleth = np.array(all_pleth_signals)\n",
    "ecg_ii = np.array(all_ecg_signals)\n",
    "\n",
    "# Create time vector\n",
    "sampling_rate = 125  # Hz\n",
    "time = np.arange(len(resp)) / sampling_rate\n",
    "\n",
    "# Calculate statistics\n",
    "total_samples = len(resp)\n",
    "total_time_minutes = (total_samples / sampling_rate) / 60\n",
    "\n",
    "print(f\"\\nüìà MULTI-SUBJECT SIGNAL STATISTICS:\")\n",
    "print(f\"\\n   RESP Signal:\")\n",
    "print(f\"     Total samples: {len(resp):,}\")\n",
    "print(f\"     Mean: {np.mean(resp):.6f}\")\n",
    "print(f\"     Std: {np.std(resp):.6f}\")\n",
    "print(f\"     Range: [{np.min(resp):.6f}, {np.max(resp):.6f}]\")\n",
    "\n",
    "print(f\"\\n   PLETH (PPG) Signal:\")\n",
    "print(f\"     Total samples: {len(pleth):,}\")\n",
    "print(f\"     Mean: {np.mean(pleth):.6f}\")\n",
    "print(f\"     Std: {np.std(pleth):.6f}\")\n",
    "print(f\"     Range: [{np.min(pleth):.6f}, {np.max(pleth):.6f}]\")\n",
    "\n",
    "print(f\"\\n   ECG II Signal:\")\n",
    "print(f\"     Total samples: {len(ecg_ii):,}\")\n",
    "print(f\"     Mean: {np.mean(ecg_ii):.6f}\")\n",
    "print(f\"     Std: {np.std(ecg_ii):.6f}\")\n",
    "print(f\"     Range: [{np.min(ecg_ii):.6f}, {np.max(ecg_ii):.6f}]\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è RECORDING TIME:\")\n",
    "print(f\"   Total samples: {total_samples:,}\")\n",
    "print(f\"   Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"   Total time: {total_time_minutes:.1f} minutes\")\n",
    "print(f\"   Avg per subject: {total_time_minutes/len(all_subjects_data):.1f} minutes\")\n",
    "\n",
    "# Store for analysis\n",
    "signals = [resp, pleth, ecg_ii]\n",
    "signal_labels = ['RESP (Respiratory)', 'PLETH (PPG)', 'ECG II']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "print(\"\\n‚úÖ Signal extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddcbc06",
   "metadata": {},
   "source": [
    "## üìä STEP 6: Visualize Raw Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 10 seconds of each signal\n",
    "duration = 10  # seconds\n",
    "samples = int(duration * sampling_rate)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# RESP signal\n",
    "axes[0].plot(time[:samples], resp[:samples], linewidth=1.5, color='blue')\n",
    "axes[0].set_title('Respiratory Signal (RESP)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PLETH signal\n",
    "axes[1].plot(time[:samples], pleth[:samples], linewidth=1.5, color='red')\n",
    "axes[1].set_title('Photoplethysmogram (PPG/PLETH)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ECG signal\n",
    "axes[2].plot(time[:samples], ecg_ii[:samples], linewidth=1.5, color='green')\n",
    "axes[2].set_title('ECG Lead II', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[2].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Displayed first {duration} seconds of each signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c16d09",
   "metadata": {},
   "source": [
    "## üìà STEP 7: Gaussian Distribution Analysis\n",
    "\n",
    "Visualize histograms and fit Gaussian curves to assess data distribution patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a02b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian distribution analysis with curve fitting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, (signal, label, color) in enumerate(zip(signals, signal_labels, colors)):\n",
    "    # Create histogram\n",
    "    axes[idx].hist(signal, bins=50, density=True, alpha=0.6, \n",
    "                   color=color, edgecolor='black', label='Data')\n",
    "    \n",
    "    # Fit Gaussian curve\n",
    "    x = np.linspace(min(signal), max(signal), 200)\n",
    "    mean_val = np.mean(signal)\n",
    "    std_val = np.std(signal)\n",
    "    gaussian_fit = stats.norm.pdf(x, mean_val, std_val)\n",
    "    \n",
    "    axes[idx].plot(x, gaussian_fit, 'r-', linewidth=2.5, \n",
    "                   label=f'Gaussian Fit\\nŒº={mean_val:.3f}\\nœÉ={std_val:.3f}')\n",
    "    \n",
    "    axes[idx].set_title(f'{label}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Signal Value', fontsize=11)\n",
    "    axes[idx].set_ylabel('Probability Density', fontsize=11)\n",
    "    axes[idx].legend(fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gaussian distribution analysis completed\")\n",
    "print(\"\\nObservation: Compare histogram shapes with fitted Gaussian curves.\")\n",
    "print(\"Close match indicates normal distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a79cb",
   "metadata": {},
   "source": [
    "## üß™ STEP 8: Shapiro-Wilk Normality Test\n",
    "\n",
    "Statistical test to determine if signals follow normal distribution.\n",
    "\n",
    "**Interpretation:** p-value > 0.05 indicates normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro-Wilk Normality Test\n",
    "print(\"=\"*80)\n",
    "print(\"SHAPIRO-WILK NORMALITY TEST\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing if signals follow normal (Gaussian) distribution...\")\n",
    "print(\"(p-value > 0.05 indicates normally distributed data)\\n\")\n",
    "\n",
    "normality_results = []\n",
    "\n",
    "for signal, label in zip(signals, signal_labels):\n",
    "    # Sample 5000 points for large datasets\n",
    "    if len(signal) > 5000:\n",
    "        sample_signal = np.random.choice(signal, 5000, replace=False)\n",
    "        stat, p_value = stats.shapiro(sample_signal)\n",
    "        print(f\"‚ö† {label}: Using 5000 random samples\")\n",
    "    else:\n",
    "        stat, p_value = stats.shapiro(signal)\n",
    "    \n",
    "    is_normal = p_value > 0.05\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Statistic = {stat:.6f}\")\n",
    "    print(f\"  p-value   = {p_value:.6e}\")\n",
    "    \n",
    "    if is_normal:\n",
    "        print(f\"  ‚úì Result: NORMALLY DISTRIBUTED (p > 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Result: NOT NORMALLY DISTRIBUTED (p ‚â§ 0.05)\")\n",
    "    \n",
    "    normality_results.append({\n",
    "        'Signal': label,\n",
    "        'Statistic': f\"{stat:.6f}\",\n",
    "        'p-value': f\"{p_value:.6e}\",\n",
    "        'Normal': '‚úì Yes' if is_normal else '‚úó No'\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NORMALITY TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "normality_df = pd.DataFrame(normality_results)\n",
    "print(normality_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Shapiro-Wilk normality testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4f5e2",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è STEP 9: Time-Domain Feature Extraction\n",
    "\n",
    "Extract 9 statistical features from each signal:\n",
    "- Mean, Std, Variance\n",
    "- Skewness, Kurtosis\n",
    "- RMS, Peak-to-Peak\n",
    "- Min, Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd46447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-domain feature extraction function\n",
    "def extract_time_features(signal, signal_name):\n",
    "    \"\"\"\n",
    "    Extract comprehensive time-domain features from physiological signal\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'Signal': signal_name,\n",
    "        'Mean': np.mean(signal),\n",
    "        'Std': np.std(signal),\n",
    "        'Variance': np.var(signal),\n",
    "        'Skewness': stats.skew(signal),\n",
    "        'Kurtosis': stats.kurtosis(signal),\n",
    "        'RMS': np.sqrt(np.mean(signal**2)),\n",
    "        'Peak-to-Peak': np.ptp(signal),\n",
    "        'Min': np.min(signal),\n",
    "        'Max': np.max(signal)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Extract features\n",
    "print(\"=\"*80)\n",
    "print(\"TIME-DOMAIN FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nExtracting time-domain features for each signal...\\n\")\n",
    "\n",
    "time_features_list = []\n",
    "\n",
    "for signal, label in zip(signals, signal_labels):\n",
    "    features = extract_time_features(signal, label)\n",
    "    time_features_list.append(features)\n",
    "    \n",
    "    print(f\"{label}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in features.items():\n",
    "        if key != 'Signal':\n",
    "            print(f\"  {key:15s}: {value:12.6f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame\n",
    "time_features_df = pd.DataFrame(time_features_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TIME-DOMAIN FEATURES SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(time_features_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "time_features_df.to_csv('time_domain_features.csv', index=False)\n",
    "print(\"\\n‚úì Time-domain features saved to 'time_domain_features.csv'\")\n",
    "print(\"‚úÖ Time-domain feature extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894b8d4",
   "metadata": {},
   "source": [
    "## üåä STEP 10: Frequency-Domain Feature Extraction\n",
    "\n",
    "Extract 6 spectral features using FFT:\n",
    "- Total Power\n",
    "- Mean/Median/Peak Frequency\n",
    "- Frequency Std\n",
    "- Spectral Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d655a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency-domain feature extraction function\n",
    "def extract_frequency_features(signal, signal_name, sampling_rate=125):\n",
    "    \"\"\"\n",
    "    Extract frequency-domain features using Fast Fourier Transform (FFT)\n",
    "    \"\"\"\n",
    "    n = len(signal)\n",
    "    \n",
    "    # Compute FFT\n",
    "    freq = fftfreq(n, d=1/sampling_rate)[:n//2]\n",
    "    fft_vals = np.abs(fft(signal))[:n//2]\n",
    "    \n",
    "    # Power Spectral Density\n",
    "    psd = fft_vals**2 / n\n",
    "    psd_norm = psd / np.sum(psd)\n",
    "    \n",
    "    # Total power\n",
    "    total_power = np.sum(psd)\n",
    "    \n",
    "    # Mean frequency\n",
    "    mean_freq = np.sum(freq * psd_norm)\n",
    "    \n",
    "    # Median frequency\n",
    "    cumsum_psd = np.cumsum(psd_norm)\n",
    "    median_freq_idx = np.where(cumsum_psd >= 0.5)[0]\n",
    "    median_freq = freq[median_freq_idx[0]] if len(median_freq_idx) > 0 else 0\n",
    "    \n",
    "    # Spectral entropy\n",
    "    spectral_entropy = -np.sum(psd_norm * np.log2(psd_norm + 1e-12))\n",
    "    \n",
    "    # Peak frequency\n",
    "    peak_freq_idx = np.argmax(psd)\n",
    "    peak_freq = freq[peak_freq_idx]\n",
    "    \n",
    "    # Frequency standard deviation\n",
    "    freq_std = np.sqrt(np.sum(((freq - mean_freq)**2) * psd_norm))\n",
    "    \n",
    "    features = {\n",
    "        'Signal': signal_name,\n",
    "        'Total_Power': total_power,\n",
    "        'Mean_Frequency': mean_freq,\n",
    "        'Median_Frequency': median_freq,\n",
    "        'Peak_Frequency': peak_freq,\n",
    "        'Frequency_Std': freq_std,\n",
    "        'Spectral_Entropy': spectral_entropy\n",
    "    }\n",
    "    \n",
    "    return features, freq, psd\n",
    "\n",
    "# Extract frequency features\n",
    "print(\"=\"*80)\n",
    "print(\"FREQUENCY-DOMAIN FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nExtracting frequency-domain features for each signal...\\n\")\n",
    "\n",
    "freq_features_list = []\n",
    "freq_data = []  # Store for plotting\n",
    "\n",
    "for signal, label in zip(signals, signal_labels):\n",
    "    features, freq, psd = extract_frequency_features(signal, label, sampling_rate=125)\n",
    "    freq_features_list.append(features)\n",
    "    freq_data.append((freq, psd, label))\n",
    "    \n",
    "    print(f\"{label}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in features.items():\n",
    "        if key != 'Signal':\n",
    "            print(f\"  {key:20s}: {value:12.6f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame\n",
    "freq_features_df = pd.DataFrame(freq_features_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FREQUENCY-DOMAIN FEATURES SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(freq_features_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "freq_features_df.to_csv('frequency_domain_features.csv', index=False)\n",
    "print(\"\\n‚úì Frequency-domain features saved to 'frequency_domain_features.csv'\")\n",
    "print(\"‚úÖ Frequency-domain feature extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960f4a3",
   "metadata": {},
   "source": [
    "## üìä STEP 11: Power Spectral Density Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e642a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Power Spectral Density for each signal\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "for idx, (freq, psd, label) in enumerate(freq_data):\n",
    "    color = colors[idx]\n",
    "    axes[idx].plot(freq, psd, linewidth=1.5, color=color)\n",
    "    axes[idx].set_title(f'Power Spectral Density - {label}', \n",
    "                        fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Frequency (Hz)', fontsize=11)\n",
    "    axes[idx].set_ylabel('Power', fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_xlim([0, 10])  # Focus on physiological range\n",
    "    \n",
    "    # Add peak frequency annotation\n",
    "    peak_idx = np.argmax(psd[:int(10*len(freq)/freq[-1])])\n",
    "    peak_freq_val = freq[peak_idx]\n",
    "    peak_power_val = psd[peak_idx]\n",
    "    axes[idx].plot(peak_freq_val, peak_power_val, 'r*', markersize=15, \n",
    "                   label=f'Peak: {peak_freq_val:.3f} Hz')\n",
    "    axes[idx].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Power Spectral Density visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862bfa25",
   "metadata": {},
   "source": [
    "## üìã STEP 12: Comprehensive Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50318154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä DATASET INFORMATION:\")\n",
    "print(f\"  Total subjects in dataset: 53\")\n",
    "print(f\"  Subjects analyzed: {len(all_subjects_data)}\")\n",
    "print(f\"  Total recording time: {total_time_minutes:.1f} minutes\")\n",
    "print(f\"  Sampling rate: {sampling_rate} Hz\")\n",
    "print(f\"  Total samples: {total_samples:,}\")\n",
    "\n",
    "print(\"\\nüî¨ STATISTICAL ANALYSIS:\")\n",
    "print(f\"  Signals analyzed: {len(signals)}\")\n",
    "print(f\"  Normality tests conducted: {len(normality_results)}\")\n",
    "normally_distributed = sum([1 for r in normality_results if '‚úì' in r['Normal']])\n",
    "print(f\"  Normally distributed: {normally_distributed}/{len(normality_results)}\")\n",
    "\n",
    "print(\"\\nüìà FEATURES EXTRACTED:\")\n",
    "print(f\"  Time-domain features per signal: {len(time_features_list[0]) - 1}\")\n",
    "print(f\"  Frequency-domain features per signal: {len(freq_features_list[0]) - 1}\")\n",
    "print(f\"  Total features per signal: {len(time_features_list[0]) + len(freq_features_list[0]) - 2}\")\n",
    "\n",
    "print(\"\\nüíæ OUTPUT FILES GENERATED:\")\n",
    "print(\"  ‚úì time_domain_features.csv\")\n",
    "print(\"  ‚úì frequency_domain_features.csv\")\n",
    "print(\"  ‚úì Visualization plots (inline)\")\n",
    "\n",
    "print(\"\\nüéØ ASSIGNMENT COMPLETION STATUS:\")\n",
    "print(\"  ‚úÖ Data loading and preprocessing\")\n",
    "print(\"  ‚úÖ Exploratory data analysis\")\n",
    "print(\"  ‚úÖ Gaussian distribution analysis\")\n",
    "print(\"  ‚úÖ Shapiro-Wilk normality testing\")\n",
    "print(\"  ‚úÖ Time-domain feature extraction\")\n",
    "print(\"  ‚úÖ Frequency-domain feature extraction\")\n",
    "print(\"  ‚úÖ Comprehensive visualization\")\n",
    "print(\"  ‚úÖ Feature hypothesis documentation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ASSIGNMENT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìù NEXT STEPS FOR SUBMISSION:\")\n",
    "print(\"  1. Verify all cells have been executed\")\n",
    "print(\"  2. Check all visualizations are displayed\")\n",
    "print(\"  3. Download generated CSV files\")\n",
    "print(\"  4. Click 'Share' ‚Üí 'Anyone with link can view'\")\n",
    "print(\"  5. Copy and submit the Colab link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275cc564",
   "metadata": {},
   "source": [
    "## üéì STEP 13: Feature Selection Hypothesis\n",
    "\n",
    "### Why These Features are Appropriate for PPG and Respiration Analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### **Time-Domain Features:**\n",
    "\n",
    "1. **Mean & RMS**: Baseline physiological levels\n",
    "   - Respiratory mean ‚Üí lung volume baseline\n",
    "   - PPG mean ‚Üí peripheral blood volume\n",
    "   - **Clinical relevance**: Detect baseline shifts in pathological conditions\n",
    "\n",
    "2. **Standard Deviation & Variance**: Signal variability\n",
    "   - High respiratory variance ‚Üí irregular breathing (sleep apnea)\n",
    "   - PPG variability ‚Üí cardiac output changes\n",
    "   - **Clinical relevance**: Arrhythmia and breathing disorder detection\n",
    "\n",
    "3. **Peak-to-Peak Amplitude**: Signal excursion\n",
    "   - Respiratory P-P ‚Üí tidal volume (breath depth)\n",
    "   - PPG P-P ‚Üí pulse pressure\n",
    "   - **Clinical relevance**: Shallow breathing or weak pulse detection\n",
    "\n",
    "4. **Skewness**: Distribution asymmetry\n",
    "   - Respiratory skewness ‚Üí asymmetric breathing (asthma/COPD)\n",
    "   - PPG skewness ‚Üí arterial stiffness indicator\n",
    "\n",
    "5. **Kurtosis**: Distribution tail behavior\n",
    "   - High kurtosis ‚Üí abnormal events (apneas, arrhythmias)\n",
    "   - **Clinical relevance**: Intermittent abnormality detection\n",
    "\n",
    "---\n",
    "\n",
    "#### **Frequency-Domain Features:**\n",
    "\n",
    "1. **Mean/Peak Frequency**: Dominant physiological rhythms\n",
    "   - Normal respiratory rate: 0.2-0.33 Hz (12-20 breaths/min)\n",
    "   - Normal heart rate: 1-1.5 Hz (60-90 bpm)\n",
    "   - **Clinical relevance**: Detect tachypnea/bradypnea, tachycardia/bradycardia\n",
    "\n",
    "2. **Spectral Entropy**: Rhythm regularity\n",
    "   - Low entropy ‚Üí regular, periodic (healthy)\n",
    "   - High entropy ‚Üí irregular, chaotic (pathological)\n",
    "   - **Clinical relevance**: Cheyne-Stokes breathing, atrial fibrillation\n",
    "\n",
    "3. **Total Power**: Overall signal energy\n",
    "   - Reduced power ‚Üí weakened physiological function\n",
    "   - **Clinical relevance**: Respiratory effort and cardiac contractility\n",
    "\n",
    "---\n",
    "\n",
    "### **Hypothesis Statement:**\n",
    "\n",
    "> *\"The combination of time-domain and frequency-domain features extracted from multi-subject PPG and respiratory signals provides a comprehensive and robust characterization of cardiorespiratory function that can effectively distinguish between normal and pathological states across a diverse population.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications:**\n",
    "\n",
    "- **Sleep Medicine**: Apnea detection using respiratory irregularity\n",
    "- **Pulmonology**: COPD/asthma monitoring via breathing patterns\n",
    "- **Cardiology**: Arrhythmia detection through heart rate variability\n",
    "- **Critical Care**: Multi-parameter patient monitoring\n",
    "- **Telemedicine**: Remote patient monitoring with robust features\n",
    "\n",
    "---\n",
    "\n",
    "### **Validation:**\n",
    "\n",
    "‚úÖ Multi-subject analysis ensures feature robustness  \n",
    "‚úÖ Statistical testing validates data characteristics  \n",
    "‚úÖ Frequency analysis reveals physiological rhythms  \n",
    "‚úÖ Clinical interpretability for each feature  \n",
    "‚úÖ Computational efficiency for real-time applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050ea51",
   "metadata": {},
   "source": [
    "## ‚úÖ CONCLUSION\n",
    "\n",
    "This assignment successfully completed statistical analysis and feature extraction on the BIDMC PPG and Respiration dataset:\n",
    "\n",
    "### **Achievements:**\n",
    "\n",
    "‚úÖ Loaded and preprocessed multi-subject physiological data  \n",
    "‚úÖ Performed comprehensive exploratory data analysis  \n",
    "‚úÖ Conducted Gaussian distribution analysis with curve fitting  \n",
    "‚úÖ Performed Shapiro-Wilk normality testing  \n",
    "‚úÖ Extracted 9 time-domain features per signal  \n",
    "‚úÖ Extracted 6 frequency-domain features per signal  \n",
    "‚úÖ Generated power spectral density visualizations  \n",
    "‚úÖ Provided clinical justification for feature selection  \n",
    "‚úÖ Created comprehensive analysis documentation\n",
    "\n",
    "---\n",
    "\n",
    "### **Skills Demonstrated:**\n",
    "\n",
    "- Python programming for biomedical signal processing\n",
    "- Statistical analysis and hypothesis testing\n",
    "- Time and frequency domain signal analysis\n",
    "- Data visualization and interpretation\n",
    "- Feature engineering for machine learning applications\n",
    "- Clinical understanding of physiological signals\n",
    "\n",
    "---\n",
    "\n",
    "### **Future Applications:**\n",
    "\n",
    "These methods form the foundation for:\n",
    "- Machine learning classification models\n",
    "- Real-time health monitoring systems\n",
    "- Disease detection algorithms\n",
    "- Telemedicine applications\n",
    "- Clinical decision support systems\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Prepared by:** [Your Name]  \n",
    "**Submission Date:** November 17, 2025  \n",
    "**Google Colab Link:** [Paste your shareable link here after sharing]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0d55d",
   "metadata": {},
   "source": [
    "## üìù SUBMISSION CHECKLIST\n",
    "\n",
    "Before submitting, verify:\n",
    "\n",
    "- [ ] All cells have been executed successfully\n",
    "- [ ] All visualizations are displayed\n",
    "- [ ] No error messages in any cell\n",
    "- [ ] Student name and ID filled in header\n",
    "- [ ] Dataset path correctly configured\n",
    "- [ ] All output files generated\n",
    "- [ ] Feature tables are complete\n",
    "- [ ] Notebook shared with \"Anyone with link can view\"\n",
    "- [ ] Shareable link copied and ready to submit\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Share on Google Colab:**\n",
    "\n",
    "1. Click the **\"Share\"** button (top-right corner)\n",
    "2. Under \"General access\", click **\"Restricted\"**\n",
    "3. Change to **\"Anyone with the link\"**\n",
    "4. Ensure role is set to **\"Viewer\"**\n",
    "5. Click **\"Copy link\"**\n",
    "6. Submit this link on **Google Classroom**\n",
    "\n",
    "---\n",
    "\n",
    "### **Grading Criteria:**\n",
    "\n",
    "- Dataset loading and preprocessing: 15%\n",
    "- Gaussian distribution analysis: 15%\n",
    "- Shapiro-Wilk normality testing: 15%\n",
    "- Time-domain features: 20%\n",
    "- Frequency-domain features: 20%\n",
    "- Visualization quality: 10%\n",
    "- Feature hypothesis: 5%\n",
    "\n",
    "**Total: 100%**\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your submission! üéì**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
